{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3469443b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\91852\\anaconda3\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: urllib3[secure]~=1.26 in c:\\users\\91852\\anaconda3\\lib\\site-packages (from selenium) (1.26.4)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\91852\\anaconda3\\lib\\site-packages (from selenium) (0.19.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\91852\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\91852\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\91852\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\91852\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.5)\n",
      "Requirement already satisfied: idna in c:\\users\\91852\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.10)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\91852\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\91852\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (20.3.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\91852\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\91852\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\91852\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.0.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\91852\\anaconda3\\lib\\site-packages (from urllib3[secure]~=1.26->selenium) (2020.12.5)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in c:\\users\\91852\\anaconda3\\lib\\site-packages (from urllib3[secure]~=1.26->selenium) (20.0.1)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in c:\\users\\91852\\anaconda3\\lib\\site-packages (from urllib3[secure]~=1.26->selenium) (3.4.7)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\91852\\anaconda3\\lib\\site-packages (from pyOpenSSL>=0.14->urllib3[secure]~=1.26->selenium) (1.15.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\91852\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.12.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5a95d6",
   "metadata": {},
   "source": [
    "Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You \n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 \n",
    "jobs data. \n",
    "This task will be done in following steps: \n",
    "1. First get the webpage https://www.naukri.com/ 2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the \n",
    "location” field. \n",
    "3. Then click the search button. \n",
    "4. Then scrape the data for the first 10 jobs results you get. \n",
    "5. Finally create a dataframe of the scraped data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1b3e27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e9be8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets now import all the requried libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3439dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets first connect to the web driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\91852\\Dropbox\\My PC (LAPTOP-APF4NTSI)\\Downloads\\chromedriver_win32/chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b79f15cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de8c9dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.naukri.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36120876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding element for job search bar\n",
    "search_job = driver.find_element_by_id('qsb-keyword-sugg')\n",
    "# job search bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b91b578",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_job.send_keys(\"Data Analyst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4f115bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn = driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "#location search bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cdfe8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifing the url of the webpage to be scraped\n",
    "url=\"https://www.naukri.com/data-analytics-jobs-in-bangalore-bengaluru?k=data%20analytics&l=bangalore%2Fbengaluru\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48ed4d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets open the webpage through our web driver\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a617507a",
   "metadata": {},
   "source": [
    "Now we will see in the window opened by webdriver whether the webpage has opened or not. lets check it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484efa79",
   "metadata": {},
   "source": [
    "So,Now lets first create 10 empty lists. In these lists the data will be stored while scraping. We have created 10 empty lists for 4 features we have to extract "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ad34b8",
   "metadata": {},
   "source": [
    "1.job_titles2.company_names3.locations_list4.experience_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0dadd251",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles=[]\n",
    "company_names=[]\n",
    "locations=[]\n",
    "experience_list=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2e88e0",
   "metadata": {},
   "source": [
    "First,we will extract all the tags where we have the jobs titles.Let me first show you on the webpae in which tags the job titles are put."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90d8847d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"0351a6f8-17f1-46cb-b869-4c1294c576b6\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"94b53808-b4b2-4501-857a-08efe635a6f3\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"2294f6a0-1da8-4888-9ea2-4fe34b6b27aa\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"a1a4b8aa-b1c4-4475-9b01-6763c10a0531\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"6c401bce-580b-4d73-9f29-5d87657ca378\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"26403101-d13d-4eeb-81a2-420c8d7aa9bb\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"662f2f0a-9d8c-43d7-9cb8-305d8549b6cc\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"05dad0a6-f883-477f-8693-0a6d9e34e370\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"729ad147-1049-48af-a4a4-ec9fe4853151\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"47653fb9-db9e-4569-8730-1ed90b1ba948\")>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so Lets extract all the tags having the jobs-titles\n",
    "titles_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "titles_tags[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fd2646",
   "metadata": {},
   "source": [
    "Now we have all the tags in which there are the job titles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f6d030",
   "metadata": {},
   "source": [
    "Now we will extract the text from these tags one by one by looping over these tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0975661a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Senior Data Scientist- Data & Analytics',\n",
       " 'Data Analyst - Flipkart Analytics',\n",
       " 'Cloud Solution Architect - Data & Analytics',\n",
       " 'Vice President- KYC Data Analytics Operations',\n",
       " 'Data Analytic Consultant- US based MNC',\n",
       " 'Data Analytics - For freshers',\n",
       " 'Data Analytics and Interpretation Application Developer',\n",
       " 'Manager - Data Analytics',\n",
       " 'H&M Group Senior Data Engineer - Analytics & Data Platforms',\n",
       " 'Lead Data Analyst - Claim Analytics']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now the text of the job title is inside the tags extracted above.\n",
    "# So we will run a loop to iterate over the tags extracted above and extract the tags\n",
    "\n",
    "for i in titles_tags:\n",
    "    title=i.text\n",
    "    job_titles.append(title)\n",
    "job_titles[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3eee9c",
   "metadata": {},
   "source": [
    "Now we will extract all the html tags where we have the company names. Lets me first show you in which tags the company names are put on the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14f70ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"f5681c73-c92c-4eed-b230-4c2d32801040\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"0492334a-62da-416a-8dca-56931d759b75\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"ec97f135-5d0d-4b90-960a-d5475395e47e\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"0e5313c9-908d-4a4a-b64c-c83fcdb8fc5a\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"a10eaac7-85f6-4d1d-a9f8-0b6adee6131a\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"40f9d502-d8d7-438e-9650-0fcb07fd2081\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"f4277c59-766f-4c8d-95f4-2a5f142c48c5\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"0a2077be-351b-4c02-995e-04f1924f0ca7\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"80840876-aafe-48e4-99c8-e045b0527d59\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"7f0eca0d-e94b-4068-8c76-759a5ce1e5cd\")>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so lets extract all the tags having the company names\n",
    "companies_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "companies_tags[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8916e9a",
   "metadata": {},
   "source": [
    "Now we have all the tags in which there are the company names. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007d30e8",
   "metadata": {},
   "source": [
    "Now we will extract the text from these tags one by one by looping over these tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1cf40cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ExecBoardinAsia',\n",
       " 'Flipkart',\n",
       " 'Accenture',\n",
       " 'Mancer Consulting Services Pvt. Ltd.',\n",
       " 'RANDSTAD INDIA PVT LTD',\n",
       " 'IANT',\n",
       " 'Accenture',\n",
       " 'LatentView Analytics Private Limited',\n",
       " 'H and M Hennes and Mauritz (P) Ltd.',\n",
       " 'InnovAccer']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in companies_tags:\n",
    "    company_name=i.text\n",
    "    company_names.append(company_name)\n",
    "company_names[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf2f0f6",
   "metadata": {},
   "source": [
    "Now we will extract all the html tags where we have the experience requried data. Let me first show you in which tags this data is put on the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4bc539d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"dddd93d1-5d91-4969-84b0-7a34396fb117\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"a6844952-66d5-4659-b665-458742a9e9b4\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"d71946a0-fc8f-49c6-9278-d5dfae3723ba\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"ee0c9d06-db57-40dd-963c-7ba21c6baf68\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"54b888a2-5570-41b8-a2f6-9d4bc61afa05\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"5702dc7f-7a80-41e4-a3e9-c9427255dbb7\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"12f4da58-dea1-4f66-8126-808c0c52e995\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"549a5835-1ffd-4ee5-aa60-4a6a5af62efd\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"140c1c14-09fe-4b29-a05b-0b0d1c7dc994\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"38d28b9b-ef39-4213-88ee-c85098072dcb\")>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so lets extract all the tags having the experience requried data\n",
    "experience_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")\n",
    "experience_tags[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7c5697",
   "metadata": {},
   "source": [
    "Now we have all the tags in there is the experience requried data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2e0db2",
   "metadata": {},
   "source": [
    "Now we will extract the text from these tags one by one by looping over these tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14d63c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5-10 Yrs',\n",
       " '0-3 Yrs',\n",
       " '12-20 Yrs',\n",
       " '10-12 Yrs',\n",
       " '4-9 Yrs',\n",
       " '0-1 Yrs',\n",
       " '4-6 Yrs',\n",
       " '8-13 Yrs',\n",
       " '6-9 Yrs',\n",
       " '7-12 Yrs']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in experience_tags:\n",
    "    experience=i.text\n",
    "    experience_list.append(experience)\n",
    "experience_list[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce170c1",
   "metadata": {},
   "source": [
    "Now we will extract all the html tags where we have the location of the job data. Let me first show you in which tags this data is put on yhe webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad1b6f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"3b189bf3-dd70-4478-9453-5cedefb04476\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"63e9dd0b-b5fd-494a-a41c-ff9da11c912a\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"767b62c0-5028-4108-b387-d122aa14c9de\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"4bbca121-249b-4d27-97ed-907c7b3973b0\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"5e8a57cf-177f-45b5-90fd-f788d0119f31\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"d8f25bbd-2dd1-4480-b177-ddf75569f002\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"c1193f8d-a620-4c4b-8607-6a25fb9df247\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"6f70dc0a-45ec-456a-b345-d5b6b5ea8b26\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"b0df7e4f-ba0a-4181-8945-a9450bc530e6\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"ecb8b53331823906c6e4abd39523066b\", element=\"4a970793-e40b-4cf1-8948-0e6be0ad1298\")>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "locations_tags[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e80573",
   "metadata": {},
   "source": [
    "Now we have all the tags in which there is the data about the location of job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38430d2",
   "metadata": {},
   "source": [
    "Now we will extract the text(location) from these tags one by one by looping over these tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78401b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru(Bellandur)',\n",
       " 'Kolkata, Mumbai, Hyderabad/Secunderabad, Pune, Chennai, Bangalore/Bengaluru, Delhi / NCR',\n",
       " 'Mumbai, Hyderabad/Secunderabad, Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Noida, Bangalore/Bengaluru']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in locations_tags :\n",
    "    location=i.text\n",
    "    locations.append(location)\n",
    "locations[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9425444b",
   "metadata": {},
   "source": [
    "So,now we have extracted the data requried from the webpage and stored them in the 4 lists mentioned above. Now before creating a dataframe from these lists. Lets first check the length of each of the list. Because if the length of all of the lists are not equal, then a dataframe cannot be formed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f026291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 20 20 20\n"
     ]
    }
   ],
   "source": [
    "print(len(job_titles),len(company_names),len(experience_list),len(locations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fbe1a46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['title']=job_titles[0:10]\n",
    "jobs['company']=company_names[0:10]\n",
    "jobs['experience_requried']=experience_list[0:10]\n",
    "jobs['location']=locations[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fadbe341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>experience_requried</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Data Scientist- Data &amp; Analytics</td>\n",
       "      <td>ExecBoardinAsia</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst - Flipkart Analytics</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru(Bellandur)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cloud Solution Architect - Data &amp; Analytics</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>12-20 Yrs</td>\n",
       "      <td>Kolkata, Mumbai, Hyderabad/Secunderabad, Pune,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vice President- KYC Data Analytics Operations</td>\n",
       "      <td>Mancer Consulting Services Pvt. Ltd.</td>\n",
       "      <td>10-12 Yrs</td>\n",
       "      <td>Mumbai, Hyderabad/Secunderabad, Bangalore/Beng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analytic Consultant- US based MNC</td>\n",
       "      <td>RANDSTAD INDIA PVT LTD</td>\n",
       "      <td>4-9 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Analytics - For freshers</td>\n",
       "      <td>IANT</td>\n",
       "      <td>0-1 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Analytics and Interpretation Application ...</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>4-6 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Manager - Data Analytics</td>\n",
       "      <td>LatentView Analytics Private Limited</td>\n",
       "      <td>8-13 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>H&amp;M Group Senior Data Engineer - Analytics &amp; D...</td>\n",
       "      <td>H and M Hennes and Mauritz (P) Ltd.</td>\n",
       "      <td>6-9 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lead Data Analyst - Claim Analytics</td>\n",
       "      <td>InnovAccer</td>\n",
       "      <td>7-12 Yrs</td>\n",
       "      <td>Noida, Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0            Senior Data Scientist- Data & Analytics   \n",
       "1                  Data Analyst - Flipkart Analytics   \n",
       "2        Cloud Solution Architect - Data & Analytics   \n",
       "3      Vice President- KYC Data Analytics Operations   \n",
       "4             Data Analytic Consultant- US based MNC   \n",
       "5                      Data Analytics - For freshers   \n",
       "6  Data Analytics and Interpretation Application ...   \n",
       "7                           Manager - Data Analytics   \n",
       "8  H&M Group Senior Data Engineer - Analytics & D...   \n",
       "9                Lead Data Analyst - Claim Analytics   \n",
       "\n",
       "                                company experience_requried  \\\n",
       "0                       ExecBoardinAsia            5-10 Yrs   \n",
       "1                              Flipkart             0-3 Yrs   \n",
       "2                             Accenture           12-20 Yrs   \n",
       "3  Mancer Consulting Services Pvt. Ltd.           10-12 Yrs   \n",
       "4                RANDSTAD INDIA PVT LTD             4-9 Yrs   \n",
       "5                                  IANT             0-1 Yrs   \n",
       "6                             Accenture             4-6 Yrs   \n",
       "7  LatentView Analytics Private Limited            8-13 Yrs   \n",
       "8   H and M Hennes and Mauritz (P) Ltd.             6-9 Yrs   \n",
       "9                            InnovAccer            7-12 Yrs   \n",
       "\n",
       "                                            location  \n",
       "0                                Bangalore/Bengaluru  \n",
       "1                     Bangalore/Bengaluru(Bellandur)  \n",
       "2  Kolkata, Mumbai, Hyderabad/Secunderabad, Pune,...  \n",
       "3  Mumbai, Hyderabad/Secunderabad, Bangalore/Beng...  \n",
       "4                                Bangalore/Bengaluru  \n",
       "5                                Bangalore/Bengaluru  \n",
       "6                                Bangalore/Bengaluru  \n",
       "7                                Bangalore/Bengaluru  \n",
       "8                                Bangalore/Bengaluru  \n",
       "9                         Noida, Bangalore/Bengaluru  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccbd914",
   "metadata": {},
   "source": [
    "Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You \n",
    "have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data. \n",
    "This task will be done in following steps: \n",
    "1. First get the webpage https://www.naukri.com/ 2. Enter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the \n",
    "location” field. \n",
    "3. Then click the search button. \n",
    "4. Then scrape the data for the first 10 jobs results you get. \n",
    "5. Finally create a dataframe of the scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9f6f319",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets now import all the requried libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "415a6b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets first connect to the web driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\91852\\Dropbox\\My PC (LAPTOP-APF4NTSI)\\Downloads\\chromedriver_win32/chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf79d268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e21eed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.naukri.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "607b9d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding element for job search bar\n",
    "search_job = driver.find_element_by_id('qsb-keyword-sugg')\n",
    "# job search bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad34a198",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_job.send_keys(\"Data Scientist\")\n",
    "#location search bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "84f7daf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn = driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8af5db0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifing the url of the webpage to be scraped\n",
    "url=\"https://www.naukri.com/data-scientist-jobs-in-bangalore-bengaluru?k=data%20scientist&l=bangalore%2Fbengaluru\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4955017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets open the webpage through our web driver\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3747e0",
   "metadata": {},
   "source": [
    "Now we will see in the window opened by webdriver whether the webpage has opened or not. lets check it\n",
    "\n",
    "So,Now lets first create 10 empty lists. In these lists the data will be stored while scraping.We have created 10 empty lists for 4 features which we have to exract\n",
    "\n",
    "1.job_titles2.company_names3.locations_list4.experience_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "759795c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles=[]\n",
    "company_names=[]\n",
    "locations=[]\n",
    "experience_list=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47294bbb",
   "metadata": {},
   "source": [
    "First, we will extract all the tags where we have the jobs titles.Let me first show you on the webpage in which tags the job titles are put."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "023f4fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"31cea803-eee3-49e6-9636-5681b2d385af\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"2c3a8ef7-5c48-4759-83e8-a8111a089205\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"6aadb6d6-9270-4937-951e-c21d358e38d9\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"64f5f1d2-17e0-47c6-8849-215b03bf088a\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"1a95d2a1-d126-48f5-9101-6987b6cdf022\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"f3fa0870-d929-467f-8d69-0de9850b1b32\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"b6c26a88-c36a-4f67-9681-a8a794749f6f\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"cf5036a2-8f64-4f36-8a3b-e5e243eb91b0\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"8a37a1b2-5a36-4ea0-a0dd-4fe4ad20e10f\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"a0be138c-0059-489a-95cb-739cf6617d51\")>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so Lets extract all the tags having the jobs-titles\n",
    "titles_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "titles_tags[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e9b4ba",
   "metadata": {},
   "source": [
    "Now we have all the tags in which there are the job titles.\n",
    "\n",
    "Now we will extract the text from these tags one by one by looping over these tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "032c2ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Senior Data Scientist',\n",
       " 'Data Scientist: Advanced Analytics',\n",
       " 'Senior Data Scientist | Fortune 500 Supermarket Chain',\n",
       " 'Senior Data Scientist',\n",
       " 'Senior Data Scientist- Data & Analytics',\n",
       " 'Forecasting Analyst/ Data Scientist (US Client)',\n",
       " 'Data Scientist',\n",
       " 'Process Innovation Analyst - APAC/Data Scientist - Third Party Role',\n",
       " 'Data Scientist',\n",
       " 'Data Scientist']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now the text of the job title is inside the tags extracted above.\n",
    "# so we will run a loop to iterate over the tags extracted above and extract the tags\n",
    "\n",
    "for i in titles_tags:\n",
    "    title=i.text\n",
    "    job_titles.append(title)\n",
    "job_titles[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a99713e",
   "metadata": {},
   "source": [
    "Now we will extract all the html tags where we have the company names.Let me first show you in which tags the company names are put on the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "81b67d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"9e7d9fc4-3c83-4c99-81fd-05fec9db1af1\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"3d26c275-5f5d-4df7-8064-15efba13e864\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"0623c367-b11a-48e0-ada2-cdae0cbb9b68\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"6ee95c36-8338-4e90-9619-e87a07134156\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"c5b4c716-e7c1-4c5b-b6ad-ddd517315720\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"aa5041c5-802f-4030-9135-0e8b05ea7492\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"76d7fe25-7bc1-4023-9de2-ceb1f2a69eda\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"a72ede06-081a-43dd-8977-13cb79219d09\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"2c1a3699-836c-4d1f-b971-8fa7f6e816db\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"6526bcdd-444f-498d-8d69-19e1eadb537e\")>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so lets extract all the tags having the company names\n",
    "companies_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "companies_tags[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e5e8d4",
   "metadata": {},
   "source": [
    "Now we have all the tags in which there are the company names.\n",
    "\n",
    "Now we will extract the text from these tags one by one by looping over these tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8bad408d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IBM India Pvt. Limited',\n",
       " 'IBM India Pvt. Limited',\n",
       " 'TALENT500 TECH (INDIA) PRIVATE LIMITED',\n",
       " 'Hitachi Ltd.',\n",
       " 'ExecBoardinAsia',\n",
       " 'Concentrix Daksh Services',\n",
       " 'Toppr',\n",
       " 'Bayer',\n",
       " 'Hitachi Ltd.',\n",
       " 'Mobiotics IT Solution Pvt Ltd']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in companies_tags:\n",
    "    company_name=i.text\n",
    "    company_names.append(company_name)\n",
    "company_names[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c94c0a",
   "metadata": {},
   "source": [
    "Now we will extract all the html tags where we have the experience requried data. Let me first show you in which tags this data is put on the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7e0eff8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"895a8eb6-760e-4315-8628-4cd41621e059\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"31011733-2dfd-4d23-86d6-9907bf5205e5\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"b17b6d34-e69e-430d-8f4b-b3f52da9b7c5\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"31902474-85e0-40c4-af7e-5c1d166e80c4\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"835b9a8e-c66c-469e-918e-2bcc1ceaf515\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"7bb49f2c-1aac-4dcb-a3f6-db1d515f03f3\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"42f783cd-1c49-40b4-a3f2-2595cf1fece6\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"1d7e4893-edf0-40ee-924c-4be1cfe5f946\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"d8fb2e81-a322-411f-99bd-0f5612c5c963\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"0a256c61-aafe-42dd-b94f-6ec910ff98f9\")>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so Lets extract all the tags having the experience requried data\n",
    "experience_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")\n",
    "experience_tags[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff0f328",
   "metadata": {},
   "source": [
    "Now we have all the tags in there is the experience requried data.\n",
    "\n",
    "Now we will extract the text from these tags one by one by looping over these tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5f2a9feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5-10 Yrs',\n",
       " '5-10 Yrs',\n",
       " '7-10 Yrs',\n",
       " '2-5 Yrs',\n",
       " '5-10 Yrs',\n",
       " '3-8 Yrs',\n",
       " '1-3 Yrs',\n",
       " '4-8 Yrs',\n",
       " '3-7 Yrs',\n",
       " '2-5 Yrs']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in experience_tags :\n",
    "    experience=i.text\n",
    "    experience_list.append(experience)\n",
    "experience_list[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d617b5",
   "metadata": {},
   "source": [
    "Now we will extract all the html tags where we have the location of the job data. Let me first show you in which tags this data is put on the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f017176f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"3ddfdf63-871d-482b-a43c-e4cbc3e8703b\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"e1cab38c-da61-4229-8f8e-3ad56e3aae1b\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"af4d5e2b-30ec-455e-82b4-2f97b6cbf034\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"80a5ee91-79fc-48b8-8348-7ae90b0f21a7\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"7068def9-1714-4215-a0d8-900ecd26a9e7\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"45c88822-7b31-4e89-8f5f-f0dd95f5bb54\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"6b967ec8-6037-4168-8ce5-9f83b48e63c1\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"33aee364-aa46-414f-85b0-d9b9f24eef85\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"ad33b506-0bea-49f8-ad99-3a4a3108b8eb\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"d330f8ffdd008a496d2040c98f9b5d40\", element=\"ccd3e77e-ccb1-433d-a618-7701fa3ac2d4\")>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "locations_tags[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be693a3c",
   "metadata": {},
   "source": [
    "Now we have all the tags in which there is the data about the location of job.\n",
    "\n",
    "Now we will extract the text(location) from these tags one by one by looping over these tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4d227863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Gurgaon/Gurugram, Bangalore/Bengaluru',\n",
       " 'Hyderabad/Secunderabad, Bangalore/Bengaluru, Mumbai (All Areas)',\n",
       " 'Bangalore/Bengaluru, Hyderabad',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru(HSR Layout)']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in locations_tags :\n",
    "    location=i.text\n",
    "    locations.append(location)\n",
    "locations[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e2f1a0",
   "metadata": {},
   "source": [
    "So,now we have extracted the data requried from the webpage and stored them in the 4 lists mentioned above. Now before creating a dataframe from these lists. Lets first check the length of each of the list. Because if the length of all of the lists are not equal, then a dataframe cannot be formed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b0f37b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 20 20 20\n"
     ]
    }
   ],
   "source": [
    "print(len(job_titles),len(company_names),len(experience_list),len(locations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e04e1538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['title']=job_titles[0:10]\n",
    "jobs['company']=company_names[0:10]\n",
    "jobs['experience_requried']=experience_list[0:10]\n",
    "jobs['location']=locations[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4e1d3e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>experience_requried</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>IBM India Pvt. Limited</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist: Advanced Analytics</td>\n",
       "      <td>IBM India Pvt. Limited</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Data Scientist | Fortune 500 Supermarke...</td>\n",
       "      <td>TALENT500 TECH (INDIA) PRIVATE LIMITED</td>\n",
       "      <td>7-10 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Hitachi Ltd.</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Senior Data Scientist- Data &amp; Analytics</td>\n",
       "      <td>ExecBoardinAsia</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Forecasting Analyst/ Data Scientist (US Client)</td>\n",
       "      <td>Concentrix Daksh Services</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "      <td>Gurgaon/Gurugram, Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Toppr</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "      <td>Hyderabad/Secunderabad, Bangalore/Bengaluru, M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Process Innovation Analyst - APAC/Data Scienti...</td>\n",
       "      <td>Bayer</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru, Hyderabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Hitachi Ltd.</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Mobiotics IT Solution Pvt Ltd</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru(HSR Layout)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                              Senior Data Scientist   \n",
       "1                 Data Scientist: Advanced Analytics   \n",
       "2  Senior Data Scientist | Fortune 500 Supermarke...   \n",
       "3                              Senior Data Scientist   \n",
       "4            Senior Data Scientist- Data & Analytics   \n",
       "5    Forecasting Analyst/ Data Scientist (US Client)   \n",
       "6                                     Data Scientist   \n",
       "7  Process Innovation Analyst - APAC/Data Scienti...   \n",
       "8                                     Data Scientist   \n",
       "9                                     Data Scientist   \n",
       "\n",
       "                                  company experience_requried  \\\n",
       "0                  IBM India Pvt. Limited            5-10 Yrs   \n",
       "1                  IBM India Pvt. Limited            5-10 Yrs   \n",
       "2  TALENT500 TECH (INDIA) PRIVATE LIMITED            7-10 Yrs   \n",
       "3                            Hitachi Ltd.             2-5 Yrs   \n",
       "4                         ExecBoardinAsia            5-10 Yrs   \n",
       "5               Concentrix Daksh Services             3-8 Yrs   \n",
       "6                                   Toppr             1-3 Yrs   \n",
       "7                                   Bayer             4-8 Yrs   \n",
       "8                            Hitachi Ltd.             3-7 Yrs   \n",
       "9           Mobiotics IT Solution Pvt Ltd             2-5 Yrs   \n",
       "\n",
       "                                            location  \n",
       "0                                Bangalore/Bengaluru  \n",
       "1                                Bangalore/Bengaluru  \n",
       "2                                Bangalore/Bengaluru  \n",
       "3                                Bangalore/Bengaluru  \n",
       "4                                Bangalore/Bengaluru  \n",
       "5              Gurgaon/Gurugram, Bangalore/Bengaluru  \n",
       "6  Hyderabad/Secunderabad, Bangalore/Bengaluru, M...  \n",
       "7                     Bangalore/Bengaluru, Hyderabad  \n",
       "8                                Bangalore/Bengaluru  \n",
       "9                    Bangalore/Bengaluru(HSR Layout)  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac13094",
   "metadata": {},
   "source": [
    "Q3: In this question you have to scrape data using the filters available on the webpage as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084b592e",
   "metadata": {},
   "source": [
    "You have to use the location and salary filter. \n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results. \n",
    "You have to scrape the job-title, job-location, company name, experience required. \n",
    "The location filter to be used is “Delhi/NCR” The salary filter to be used is “3-6” lakhs \n",
    "The task will be done as shown in the below steps: \n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field. \n",
    "3. Then click the search button. \n",
    "4. Then apply the location filter and salary filter by checking the respective boxes \n",
    "5. Then scrape the data for the first 10 jobs results you get. \n",
    "6. Finally create a dataframe of the scraped data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f8e521ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b32c4374",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets now import all the requried libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8519d99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets first connect to the web driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\91852\\Dropbox\\My PC (LAPTOP-APF4NTSI)\\Downloads\\chromedriver_win32/chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cee99e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "51f50ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.naukri.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d4f79b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding element for job search bar\n",
    "search_job = driver.find_element_by_id('qsb-keyword-sugg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "12827130",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_job.send_keys(\"Data Scientist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "57e75830",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn = driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7c23fe40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifing the url of the webpage to be scraped\n",
    "url=\"https://www.naukri.com/data-scientist-jobs?k=data%20scientist&ctcFilter=3to6&cityTypeGid=9508\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bab1b1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets open the webpage through our web driver\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546d3e98",
   "metadata": {},
   "source": [
    "Now we will see in the window opened by webdriver the webpage has opened or not.lets check it\n",
    "\n",
    "So, Now lets first create 10 empty lists.In these lists the data will be stored while scraping.We have created 10 empty lists for 4 features which we have to extract\n",
    "\n",
    "1.job_titles2.company_names3.locations_list4.experience_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e9b82bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles=[]\n",
    "company_names=[]\n",
    "locations=[]\n",
    "experience_list=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f5f5a3",
   "metadata": {},
   "source": [
    "First,we will extract all the tags where we have the jobs titles.Let me first show you on the webpage in which tags the job titles are put."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "88ed234c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"8cf211e8-5bbc-4341-ab8d-1aafaff1a5d5\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"ee2305cb-1dcd-4e52-9229-a2b109afd4f1\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"8dad4ad5-6bdd-4366-bd71-5b930bb110fb\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"ed954f38-1397-474d-b733-5832d9313c62\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"bb8be4d7-a289-4d4a-b078-1495da77688c\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"c5e797bc-572b-44aa-93fe-ec4cd23976ea\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"5322394a-172d-43bc-9910-37b2ff5c9b45\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"759bf945-1dc0-4995-880f-3bdbd25728e2\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"71df8d1a-33c6-44ba-b4f6-6fb7ebc09a86\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"6a79f8a7-6d15-4bfa-8248-bd8f14922740\")>]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so Lets extract all the tags having the jobs-titles\n",
    "titles_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "titles_tags[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835dbe38",
   "metadata": {},
   "source": [
    "Now we have all the tags in which there are the job titles.\n",
    "\n",
    "Now we will extract the text from these tags one by one by looping over these tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dfa706ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data Scientist - Insurance',\n",
       " 'Job Opportunity || Data Scientist || HCL Technologies',\n",
       " 'Data Scientist',\n",
       " 'Data Scientist',\n",
       " 'Data Scientist III-2',\n",
       " 'Data Scientist',\n",
       " 'Data Scientist',\n",
       " 'Urgent Hiring || Data Scientist || Delhi',\n",
       " 'Immediate requirement For Data Scientist',\n",
       " 'Data Scientist Internship']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now the text of the job title is inside the tags extracted above.\n",
    "# so we will run a loop to iterate over the tags extracted above and extract the tags\n",
    "\n",
    "for i in titles_tags:\n",
    "    title=i.text\n",
    "    job_titles.append(title)\n",
    "job_titles[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3bcdf8",
   "metadata": {},
   "source": [
    "Now we will extract all the html tags where we have the company names.Let me first show you in which tags the company names are put on the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "810fdc04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"972d1e54-1ac5-468a-9018-1c601538fa0f\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"636e84a7-c974-4b06-8d15-aec0f6b3e056\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"19bdc332-2d02-4550-bdf6-f255e7d974e8\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"d9b1d8aa-42f8-4db1-ae41-ce620ef630fc\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"0473847d-0dcb-4f6e-8825-c884abeedb7c\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"9cd3cb6f-726c-4779-93e1-a281de46e0aa\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"ff8c1f9e-1984-4d5d-88d7-af3acd8053ff\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"a3d75556-1a2e-41dc-b8c9-734c1f7a7f1f\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"937a66aa-d73a-43c5-ac35-17aa7f1bd431\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"a93b8671-af63-4953-aba4-2fddd36210f8\")>]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so lets extract all the tags having the company names\n",
    "companies_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "companies_tags[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d1e32c",
   "metadata": {},
   "source": [
    "Now we have all the tags in which there are the company names.\n",
    "\n",
    "Now we will extract the text from these tags one by one looping over these tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9d90bafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Huquo Consulting Pvt. Ltd',\n",
       " 'HCL Technologies',\n",
       " 'Think i',\n",
       " 'ThinkBumblebee Analytics Pvt. Ltd.',\n",
       " 'Concentrix Daksh Services',\n",
       " 'MoMagic Technologies Pvt. Ltd.',\n",
       " 'MoMagic Technologies Pvt. Ltd.',\n",
       " 'Shriram Automall India Limited',\n",
       " 'CALIBEHR BUSINESS SUPPORT SERVICES PRIVATE LIMITED',\n",
       " 'iHackers Inc']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in companies_tags:\n",
    "    company_name=i.text\n",
    "    company_names.append(company_name)\n",
    "company_names[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddd352d",
   "metadata": {},
   "source": [
    "Now we will extract all the html tags where we have the experience requried data.Let me first show you in which tags this data is put on the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "19654e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"cc460e2d-c832-4168-9d57-0047d324aacf\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"ed55df92-a4b0-417e-984c-0adb44402e7a\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"1937334f-5a78-43da-8963-cf8ea5ad70cd\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"15a8ff9d-1bbf-4bd6-977d-8cf155686901\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"a042aede-302f-48f8-8787-0a98cec0a473\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"c5475831-fba6-40ef-bed0-4190220090c5\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"1837c486-9d06-4d3f-916a-cddf46463bed\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"9dd8d06c-f768-4610-8dd1-b1b632811381\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"3891fd6a-f18c-4e47-a2f5-a4fd5dd2c8c5\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"7884a99d-d538-4d1b-a3d2-5f2e899a3387\")>]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so Lets extract all the tags having the experience requried data\n",
    "experience_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")\n",
    "experience_tags[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e052a10",
   "metadata": {},
   "source": [
    "Now we have all the tags in there is the experience requried data.\n",
    "\n",
    "Now we will extract the text from these tags one by one by looping over these tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cd96d2eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2-7 Yrs',\n",
       " '4-7 Yrs',\n",
       " '0-2 Yrs',\n",
       " '2-6 Yrs',\n",
       " '3-8 Yrs',\n",
       " '4-6 Yrs',\n",
       " '4-6 Yrs',\n",
       " '2-7 Yrs',\n",
       " '2-7 Yrs',\n",
       " '0-1 Yrs']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in experience_tags:\n",
    "    experience=i.text\n",
    "    experience_list.append(experience)\n",
    "experience_list[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee12d8b",
   "metadata": {},
   "source": [
    "Now we will extract all the html tags where we have the location of the job data.Let me first show you in which tags this data is put on the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "08a7f53c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"c4a2ebdd-e55b-4cc1-a2db-6c6842d50529\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"425f052d-7064-421c-8356-2ed38be7261f\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"3e01bf9a-2cdf-4d65-a70d-3164a4ed7eff\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"5357e4c0-b2b3-4341-9ab8-d1bae4b51630\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"75f94b70-39c4-4015-a340-76d0f4d79feb\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"e3551eac-0e78-445b-9a7b-aa62db1cd304\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"061e7c53-e37a-44bf-8ae8-0cd89c2ab428\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"a2ac582a-6f3e-4a9b-a44e-2fb391eabb9a\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"70ec7bb3-6810-4f61-a0d3-56a787e81488\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3670c5ef00fbdfc7420dcbacbbaef269\", element=\"d8c33a95-8af4-462b-ba0b-f857f063924b\")>]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "locations_tags[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d788dfa",
   "metadata": {},
   "source": [
    "Now we have all the tags in which there is the data about the location of job.\n",
    "\n",
    "Now we will extract the text(location) from these tags one by one by looping over these tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "41ffcaf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Noida, Gurgaon/Gurugram',\n",
       " 'Delhi / NCR',\n",
       " 'Kochi/Cochin, Kolkata, Hyderabad/Secunderabad, Pune, Ahmedabad, Chennai, Bangalore/Bengaluru, Delhi / NCR, Mumbai (All Areas)',\n",
       " 'Pune, Bangalore/Bengaluru, Delhi / NCR',\n",
       " 'Gurgaon/Gurugram',\n",
       " 'Noida(Sector-126 Noida)',\n",
       " 'Noida(Sector-126 Noida)',\n",
       " 'Delhi / NCR',\n",
       " 'Mumbai, Hyderabad/Secunderabad, Pune, Chennai, Tamia, Bangalore/Bengaluru, Delhi / NCR',\n",
       " 'New Delhi']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in locations_tags:\n",
    "    location=i.text\n",
    "    locations.append(location)\n",
    "locations[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62deb5ea",
   "metadata": {},
   "source": [
    "So,now we have extracted the data requried from the webpage and stored them in the 4 lists mentioned above.Now before creating a dataframe from these lists.Lets first check the length of each of the list. Because if the length of all of the lists are not equal, then a dataframe cannot be formed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "524730ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 20 20 20\n"
     ]
    }
   ],
   "source": [
    "print(len(job_titles),len(company_names),len(experience_list),len(locations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "331af013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['title']=job_titles[0:10]\n",
    "jobs['company']=company_names[0:10]\n",
    "jobs['experience_requried']=experience_list[0:10]\n",
    "jobs['location']=locations[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2eaf2501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>experience_requried</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist - Insurance</td>\n",
       "      <td>Huquo Consulting Pvt. Ltd</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "      <td>Noida, Gurgaon/Gurugram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Job Opportunity || Data Scientist || HCL Techn...</td>\n",
       "      <td>HCL Technologies</td>\n",
       "      <td>4-7 Yrs</td>\n",
       "      <td>Delhi / NCR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Think i</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "      <td>Kochi/Cochin, Kolkata, Hyderabad/Secunderabad,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>ThinkBumblebee Analytics Pvt. Ltd.</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "      <td>Pune, Bangalore/Bengaluru, Delhi / NCR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist III-2</td>\n",
       "      <td>Concentrix Daksh Services</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>MoMagic Technologies Pvt. Ltd.</td>\n",
       "      <td>4-6 Yrs</td>\n",
       "      <td>Noida(Sector-126 Noida)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>MoMagic Technologies Pvt. Ltd.</td>\n",
       "      <td>4-6 Yrs</td>\n",
       "      <td>Noida(Sector-126 Noida)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Urgent Hiring || Data Scientist || Delhi</td>\n",
       "      <td>Shriram Automall India Limited</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "      <td>Delhi / NCR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Immediate requirement For Data Scientist</td>\n",
       "      <td>CALIBEHR BUSINESS SUPPORT SERVICES PRIVATE LIM...</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "      <td>Mumbai, Hyderabad/Secunderabad, Pune, Chennai,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist Internship</td>\n",
       "      <td>iHackers Inc</td>\n",
       "      <td>0-1 Yrs</td>\n",
       "      <td>New Delhi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                         Data Scientist - Insurance   \n",
       "1  Job Opportunity || Data Scientist || HCL Techn...   \n",
       "2                                     Data Scientist   \n",
       "3                                     Data Scientist   \n",
       "4                               Data Scientist III-2   \n",
       "5                                     Data Scientist   \n",
       "6                                     Data Scientist   \n",
       "7           Urgent Hiring || Data Scientist || Delhi   \n",
       "8           Immediate requirement For Data Scientist   \n",
       "9                          Data Scientist Internship   \n",
       "\n",
       "                                             company experience_requried  \\\n",
       "0                          Huquo Consulting Pvt. Ltd             2-7 Yrs   \n",
       "1                                   HCL Technologies             4-7 Yrs   \n",
       "2                                            Think i             0-2 Yrs   \n",
       "3                 ThinkBumblebee Analytics Pvt. Ltd.             2-6 Yrs   \n",
       "4                          Concentrix Daksh Services             3-8 Yrs   \n",
       "5                     MoMagic Technologies Pvt. Ltd.             4-6 Yrs   \n",
       "6                     MoMagic Technologies Pvt. Ltd.             4-6 Yrs   \n",
       "7                     Shriram Automall India Limited             2-7 Yrs   \n",
       "8  CALIBEHR BUSINESS SUPPORT SERVICES PRIVATE LIM...             2-7 Yrs   \n",
       "9                                       iHackers Inc             0-1 Yrs   \n",
       "\n",
       "                                            location  \n",
       "0                            Noida, Gurgaon/Gurugram  \n",
       "1                                        Delhi / NCR  \n",
       "2  Kochi/Cochin, Kolkata, Hyderabad/Secunderabad,...  \n",
       "3             Pune, Bangalore/Bengaluru, Delhi / NCR  \n",
       "4                                   Gurgaon/Gurugram  \n",
       "5                            Noida(Sector-126 Noida)  \n",
       "6                            Noida(Sector-126 Noida)  \n",
       "7                                        Delhi / NCR  \n",
       "8  Mumbai, Hyderabad/Secunderabad, Pune, Chennai,...  \n",
       "9                                          New Delhi  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41ce2a9",
   "metadata": {},
   "source": [
    "Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes: \n",
    "1. Brand \n",
    "2. Product Description \n",
    "3. Price \n",
    "The attributes which you have to scrape is ticked marked in the below image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e4bf9801",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets now import all the requried libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f1ac0717",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets first connect to the web driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\91852\\Dropbox\\My PC (LAPTOP-APF4NTSI)\\Downloads\\chromedriver_win32/chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1090a871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7e1fc230",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.flipkart.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "91f72718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding element for product search bar\n",
    "product = driver.find_element_by_class_name(\"_3704LK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "49cf9266",
   "metadata": {},
   "outputs": [],
   "source": [
    "product.send_keys(\"sunglasses\")\n",
    "#location search bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "177bcb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn = driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "eb83002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifing the url of the webpage to be scraped\n",
    "url=\"https://www.flipkart.com/search?q=sunglasses&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "96f511b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets open the webpage through our web driver\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2730be2b",
   "metadata": {},
   "source": [
    "Now we will see in the window opened by webdriver whether the webpage has opened or not. lets check it\n",
    "\n",
    "So,Now lets first create 100 empty lists. In these lists the data will be stored while scraping.We have created 100 empty lists for 3 features which we have to exract\n",
    "\n",
    "1.Brand2.Description3.Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7d1bb0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the empty list\n",
    "brand=[]\n",
    "Description=[]\n",
    "price=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "990c3257",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the empty list\n",
    "brand=[]\n",
    "description=[]\n",
    "price=[]\n",
    "\n",
    "start=0\n",
    "end=4\n",
    "for page in range(start,end):\n",
    "#for loop for scrapping 4 page\n",
    "    brands=driver.find_elements_by_class_name('_2WkVRV')\n",
    "#scraping brands name by class name='_2WkVRV'\n",
    "    for i in brands:\n",
    "        brand.append(i.text)\n",
    "#appending the text in Brand list\n",
    "        prices=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "#scraping the price from the xpath\n",
    "    for i in prices:\n",
    "        price.append(i.text)\n",
    "#appending the text in price list\n",
    "    desc=driver.find_elements_by_xpath('//a[@class=\"IRpwTa\"]')\n",
    "#scraping description from the xpath\n",
    "    for i in desc:\n",
    "        description.append(i.text)\n",
    "#appending the text in description list\n",
    "    nxt_button=driver.find_elements_by_xpath(\"//a[@class='_1LKTO3']\")\n",
    "#scraping the list of buttons from the page\n",
    "    try:\n",
    "        driver.get(nxt_button[1].get_attribute('href'))\n",
    "#getting the link from the list for next page\n",
    "    except:\n",
    "        driver.get(nxt_button[0].get_attribute('href'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6782f4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dataframe\n",
    "df=pd.DataFrame({'Brand':brand[:100],\n",
    "                'Description':description[:100],\n",
    "                'Price':price[:100]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c29cee7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Description</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Singco India</td>\n",
       "      <td>Gradient, Toughened Glass Lens, UV Protection ...</td>\n",
       "      <td>₹699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>Polarized, UV Protection Wayfarer Sunglasses (57)</td>\n",
       "      <td>₹799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SRPM</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (56)</td>\n",
       "      <td>₹188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SUNBEE</td>\n",
       "      <td>UV Protection, Polarized, Mirrored Round Sungl...</td>\n",
       "      <td>₹276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>elegante</td>\n",
       "      <td>UV Protection Round Sunglasses (53)</td>\n",
       "      <td>₹359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>ROYAL SON</td>\n",
       "      <td>UV Protection Aviator, Wayfarer Sunglasses (Fr...</td>\n",
       "      <td>₹711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>PHENOMENAL</td>\n",
       "      <td>UV Protection, Others Rectangular Sunglasses (50)</td>\n",
       "      <td>₹249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>DEIXELS</td>\n",
       "      <td>UV Protection, Gradient Oval Sunglasses (58)</td>\n",
       "      <td>₹229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Anemone</td>\n",
       "      <td>Gradient, UV Protection Wayfarer Sunglasses (F...</td>\n",
       "      <td>₹799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Brand                                        Description Price\n",
       "0    Singco India  Gradient, Toughened Glass Lens, UV Protection ...  ₹699\n",
       "1   VINCENT CHASE  Polarized, UV Protection Wayfarer Sunglasses (57)  ₹799\n",
       "2            SRPM             UV Protection Wayfarer Sunglasses (56)  ₹188\n",
       "3          SUNBEE  UV Protection, Polarized, Mirrored Round Sungl...  ₹276\n",
       "4        Fastrack   UV Protection Rectangular Sunglasses (Free Size)  ₹639\n",
       "..            ...                                                ...   ...\n",
       "95       elegante                UV Protection Round Sunglasses (53)  ₹359\n",
       "96      ROYAL SON  UV Protection Aviator, Wayfarer Sunglasses (Fr...  ₹711\n",
       "97     PHENOMENAL  UV Protection, Others Rectangular Sunglasses (50)  ₹249\n",
       "98        DEIXELS       UV Protection, Gradient Oval Sunglasses (58)  ₹229\n",
       "99        Anemone  Gradient, UV Protection Wayfarer Sunglasses (F...  ₹799\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#printing dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b9d344",
   "metadata": {},
   "source": [
    "Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the \n",
    "search field. \n",
    "You have to scrape 4 attributes of each sneaker: \n",
    "1. Brand \n",
    "2. Product Description \n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3e6c73c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f6c000bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets now import all the requried libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5ae01e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets first connect to the web driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\91852\\Dropbox\\My PC (LAPTOP-APF4NTSI)\\Downloads\\chromedriver_win32/chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "99a31e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1b8b87a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.flipkart.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d0049043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding element for product search bar\n",
    "product = driver.find_element_by_class_name(\"_3704LK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c41726ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "product.send_keys(\"sneakers\")\n",
    "#location search bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8d9d9e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn = driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "44e1ae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifing the url of the webpage to be scraped\n",
    "url=\"https://www.flipkart.com/search?q=sneakers&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9152b7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets open the webpage through our web driver\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65eaa114",
   "metadata": {},
   "source": [
    "Now we will see in the window opened by webdriver whether the webpage has opened or not.\n",
    "lets check it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a55916e",
   "metadata": {},
   "source": [
    "So,Now lets first create 100 empty lists. In these lists the data will be stored while scraping.We have created 100 empty lists for 3 features which we have to exract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fcba66",
   "metadata": {},
   "source": [
    "1.brand2.description3.Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "98b8f8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the empty list\n",
    "brand=[]\n",
    "description=[]\n",
    "price=[]\n",
    "\n",
    "start=0\n",
    "end=4\n",
    "for page in range(start,end):\n",
    "    #for loop for scrapping 4 page\n",
    "    \n",
    "    brands=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "    #scraping title tags by xpath\n",
    "    for i in brands:\n",
    "        brand.append(i.text)\n",
    "        #appending the text in brand\n",
    "        \n",
    "        price_tags=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "        #scraping the price from the xpath\n",
    "        for i in price_tags:\n",
    "            price.append(i.text)\n",
    "            #appending the text in price tags\n",
    "            \n",
    "            description=driver.find_elements_by_xpath(\"//span[@class='B_NuCI']\")\n",
    "            #scraping the description from the xpath\n",
    "            for i in description:\n",
    "                description.append(i.text)\n",
    "                #appending the text in description\n",
    "                \n",
    "                nxt_button=driver.find_elements_by_xpath(\"//a[@class='_1LKTO3']\")\n",
    "                #scraping the list of buttons from the page\n",
    "                try:\n",
    "                    driver.get(nxt_button[1].get_attribute('href'))\n",
    "                 #getting the link from the list for next page \n",
    "                except:\n",
    "                    driver.get(nxt_button[0].get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c78f625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dataframe\n",
    "df=pd.DataFrame({'brand':brand[:100],\n",
    "                'description':'description'[:100],\n",
    "                'price':price[:100]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "732804a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Numenzo</td>\n",
       "      <td>description</td>\n",
       "      <td>₹479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SCATCHITE</td>\n",
       "      <td>description</td>\n",
       "      <td>₹398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Magnolia</td>\n",
       "      <td>description</td>\n",
       "      <td>₹398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CALCADOS</td>\n",
       "      <td>description</td>\n",
       "      <td>₹899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D-SNEAKERZ</td>\n",
       "      <td>description</td>\n",
       "      <td>₹348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Arohi</td>\n",
       "      <td>description</td>\n",
       "      <td>₹458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>description</td>\n",
       "      <td>₹699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>T-ROCK</td>\n",
       "      <td>description</td>\n",
       "      <td>₹399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Labbin</td>\n",
       "      <td>description</td>\n",
       "      <td>₹399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Kreverse</td>\n",
       "      <td>description</td>\n",
       "      <td>₹245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         brand  description price\n",
       "0      Numenzo  description  ₹479\n",
       "1    SCATCHITE  description  ₹398\n",
       "2     Magnolia  description  ₹398\n",
       "3     CALCADOS  description  ₹899\n",
       "4   D-SNEAKERZ  description  ₹348\n",
       "..         ...          ...   ...\n",
       "95       Arohi  description  ₹458\n",
       "96      Chevit  description  ₹699\n",
       "97      T-ROCK  description  ₹399\n",
       "98      Labbin  description  ₹399\n",
       "99    Kreverse  description  ₹245\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#printing dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6759e9e8",
   "metadata": {},
   "source": [
    "Q8: Go to webpage https://www.amazon.in \n",
    "Enter “Laptop” in the search field and then click the search icon.\n",
    "Then set CPU Type filter to “Intel Core i7” and “Intel Core i9”\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop: \n",
    "1. Title \n",
    "2. Ratings \n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d5510ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65d438a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets now import all the requried librares\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40301a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets first connect the webdriver \n",
    "driver=webdriver.Chrome(r\"C:\\Users\\91852\\Dropbox\\My PC (LAPTOP-APF4NTSI)\\Downloads\\chromedriver_win32/chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2c89212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99a279ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.amazon.in/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60ad04b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding element for job search bar\n",
    "search_job = driver.find_element_by_id('twotabsearchtextbox')\n",
    "# job search bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1f97640",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_job.send_keys(\"laptop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35484404",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn = driver.find_element_by_xpath(\"//span[@class='nav-search-submit-text nav-sprite nav-progressive-attribute']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19655bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifing the url of the webpage to be scraped\n",
    "url=\"https://www.amazon.in/s?k=laptop&i=computers&rh=n%3A1375424031%2Cp_n_feature_thirteen_browse-bin%3A12598163031%7C16757432031&dc&qid=1636161846&rnid=12598141031&ref=sr_nr_p_n_feature_thirteen_browse-bin_27\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "516ee311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets open the webpage through our web driver\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638bcfe4",
   "metadata": {},
   "source": [
    "Now we will see in the window opened by webdriver whether the webpage has opened or not.\n",
    "lets check it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0afc93d",
   "metadata": {},
   "source": [
    "So,Now lets first create 10 empty lists. In these lists the data will be stored while scraping.We have created 10 empty lists for 3 features which we have to exract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe2b146",
   "metadata": {},
   "source": [
    "1.title nams 2.Ratings 3.Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0685da84",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_names=[]\n",
    "Ratings=[]\n",
    "Price=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6dca6a",
   "metadata": {},
   "source": [
    "First, we will extract all the tags where we have the title name.Let me first show you on the webpage in which tags the title name are put."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20c1ea2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"5b208f23d07d8befc87d4de7b89ce890\", element=\"f90c205d-2314-485e-9894-eb7bb20b7387\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5b208f23d07d8befc87d4de7b89ce890\", element=\"50a0029e-fbb1-44a5-8c51-d3858bb9647a\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5b208f23d07d8befc87d4de7b89ce890\", element=\"ea67204d-42f3-4cd0-abba-a91b7f025ccd\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5b208f23d07d8befc87d4de7b89ce890\", element=\"95381c35-4321-4a44-aa28-769ca0c16edf\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5b208f23d07d8befc87d4de7b89ce890\", element=\"b656b64d-de9b-4f86-9b3f-a9ced5a1296d\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5b208f23d07d8befc87d4de7b89ce890\", element=\"5983a332-f5a5-47e0-a596-c77673ee7ef5\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5b208f23d07d8befc87d4de7b89ce890\", element=\"07322521-18b4-4597-8ee1-154b26d4c448\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5b208f23d07d8befc87d4de7b89ce890\", element=\"b653ea6a-3aaf-4040-980d-87ae605ac5b6\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5b208f23d07d8befc87d4de7b89ce890\", element=\"b375d2b3-5c6f-4577-b244-862e777cb75b\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5b208f23d07d8befc87d4de7b89ce890\", element=\"3f0edaa9-bd79-4de5-a92d-84e9c9d0b31a\")>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so Lets extract all the tags having the title-tags\n",
    "title_tags=driver.find_elements_by_xpath(\"//span[@class='a-size-medium a-color-base a-text-normal']\")\n",
    "title_tags[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc013202",
   "metadata": {},
   "source": [
    "Now we have all the tags in which there are the title names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e74e35",
   "metadata": {},
   "source": [
    "Now we will extract the text from these tags one by one by looping over these tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9e27da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mi Notebook Horizon Edition 14 Intel Core i7-10510U 10th Gen Thin and Light Laptop(8GB/512GB SSD/Windows 10/Nvidia MX350 2GB Graphics/Grey/1.35Kg)(Without Webcam) XMA1904-AF',\n",
       " 'HP Envy Intel 11th Gen Core i7 Processor 13.3 inches FHD Touchscreen Gaming Laptop (16GB/1TB SSD/Windows 10/NVIDIA MX450 2GB/Natural Silver/1.3 kg), 13-ba1018TX',\n",
       " 'ASUS TUF Gaming F17 (2021), 17.3-inch (43.94 cms) FHD 144Hz, Intel Core i7-11800H 11th Gen, GeForce RTX 3050 Ti 4GB Graphics, Gaming Laptop (16GB/1TB SSD/Windows 10/Eclipse Gray/2.6 Kg) FX766HE-HX022T',\n",
       " 'Mi Notebook Horizon Edition 14 Intel Core i7-10510U 10th Gen Thin and Light Laptop(8GB/512GB SSD/Windows 10/Nvidia MX350 2GB Graphics/Grey/1.35Kg)(Without Webcam) XMA1904-AF',\n",
       " 'HP Pavilion (2021) Intel 11th Gen Core i7 14 inches FHD Screen Thin & Light Laptop, 16 GB RAM, 1TB SSD, Iris Xe Graphics, Windows 10, MS Office, Backlit Keyboard, 1.41kg (14-dv0058TU)',\n",
       " 'ASUS TUF Dash F15 (2021), 15.6-inch (39.62 cms) FHD 144Hz, Intel Core i7-11370H 11th Gen, RTX 3050 4GB Graphics Gaming Laptop (16GB RAM/512GB SSD/Windows 10/White/2 kg), FX516PC-HN062T',\n",
       " 'Dell 14 (2021) i7-1195G7 2in1 Touch Screen Laptop, 16GB, 512GB SSD, Win 10 + MS Office, 14.0\" (35.56 cms) FHD Display, Backlit KB, FPR + Active Pen, Platinum Silver (Inspiron 5410, D560596WIN9S)',\n",
       " 'Dell 15 (2021) i7-10870H Gaming Laptop, 16GB DDR4, 512GB SSD, Win 10 + MS Office, NVIDIA RTX 3050 Ti 4GB, 15.6\" (39.61 cms) FHD AG 250 nits 120Hz, Backlit KB Orange (G15 5510, D560534WIN9B)',\n",
       " 'Lenovo ThinkBook 15 Intel 11th Gen Core i7 15.6\"(39.62 cm)FHD IPS 300 nits Antiglare 100% sRGB Thin & Light Laptop(16GB/1TB HDD+128GB SSD/Windows 10/MS Office/3 Yr Onsite Warranty/1.7 Kg) 20VEA0HBIH',\n",
       " 'Acer Nitro 5 AN515-57 Gaming Laptop | Intel Core i7-11800H |NVIDIA GeForce RTX 3050 Ti Laptop Graphics |15.6\" FHD 144Hz IPS Display |16GB DDR4 |256GB SSD+1TB HDD |Killer Wi-Fi 6 |RGB Backlit Keyboard']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now the text of the title_name is inside the tags extracted above.\n",
    "# so we will run a loop to iterate over the tags extracted above and extract the tags\n",
    "\n",
    "for i in title_tags:\n",
    "    title=i.text\n",
    "    title_names.append(title)\n",
    "title_names[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fb88e5",
   "metadata": {},
   "source": [
    "Now we will extract all the html tags where we have the Rating_tags data. Let me first show you in which tags this data is put on the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e7afb7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"5b208f23d07d8befc87d4de7b89ce890\", element=\"2d9e0778-c5fc-4f09-8dfb-446fceaa3e3c\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5b208f23d07d8befc87d4de7b89ce890\", element=\"7106374a-84f7-4c92-ae94-bc0b137bd211\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5b208f23d07d8befc87d4de7b89ce890\", element=\"a62ac8d4-1db6-4d44-8d0f-12390225c418\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5b208f23d07d8befc87d4de7b89ce890\", element=\"73a9e590-afa1-4bf7-81d1-c106b0541c4a\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5b208f23d07d8befc87d4de7b89ce890\", element=\"cd688af6-6c49-4487-ada3-377739c6c28f\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5b208f23d07d8befc87d4de7b89ce890\", element=\"f588b611-bfa5-4019-a05a-4d3bf98ed1e4\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5b208f23d07d8befc87d4de7b89ce890\", element=\"0674c6d4-e90d-4efb-a20a-0b353317e390\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5b208f23d07d8befc87d4de7b89ce890\", element=\"3a29c829-1e0b-46a5-bccc-86f9c58674ba\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5b208f23d07d8befc87d4de7b89ce890\", element=\"9491e481-0afc-4336-8016-5d909a2e44e8\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5b208f23d07d8befc87d4de7b89ce890\", element=\"47aa341c-d558-4cda-8eee-3c5ff28d81d8\")>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so Lets extract all the tags having the Rating\n",
    "Rating_tags=driver.find_elements_by_xpath(\"//span[@class='a-icon-alt']\")\n",
    "Rating_tags[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7286c73",
   "metadata": {},
   "source": [
    "Now we have all the tags in there is the experience requried data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430b9f4b",
   "metadata": {},
   "source": [
    "Now we will extract the text from these tags one by one by looping over these tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "637659b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', '', '', '', '', '', '', '', '']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in Rating_tags:\n",
    "    Rating=i.text\n",
    "    Ratings.append(i.text)\n",
    "Ratings[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0231f8",
   "metadata": {},
   "source": [
    "Now we will extract all the tags having the price requried data.Let me first show you in which tags this data is put on the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "774b5844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"5b208f23d07d8befc87d4de7b89ce890\", element=\"cd8cd83f-03a9-4e66-8d2b-1a98e934ef32\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5b208f23d07d8befc87d4de7b89ce890\", element=\"98cd8895-ed9e-4830-9758-fbc7eea91b1e\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5b208f23d07d8befc87d4de7b89ce890\", element=\"8d21acbe-49ea-487f-b7a7-5b32e25c25db\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5b208f23d07d8befc87d4de7b89ce890\", element=\"df50654b-3ddd-404a-be8e-6a0431998a8a\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5b208f23d07d8befc87d4de7b89ce890\", element=\"347cacb8-94a5-4cd3-ba88-ae4b690b0332\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5b208f23d07d8befc87d4de7b89ce890\", element=\"9c2bec20-ff9b-419e-b5d1-c67cf6a3d14e\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5b208f23d07d8befc87d4de7b89ce890\", element=\"0f6a241a-5e1b-41ac-8fc9-d1f536f0b6fb\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5b208f23d07d8befc87d4de7b89ce890\", element=\"c68df098-3c17-453f-9e76-d29e03ced9c5\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5b208f23d07d8befc87d4de7b89ce890\", element=\"4d2bbc67-b483-46b5-8a7d-4ed71f9a9653\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"5b208f23d07d8befc87d4de7b89ce890\", element=\"a4c3ce6f-0786-4db0-bef2-1efb34d4271c\")>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so Lets extract all the tags having the price requried data\n",
    "price_tags=driver.find_elements_by_xpath(\"//span[@class='a-price-whole']\")\n",
    "price_tags[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d823743",
   "metadata": {},
   "source": [
    "Now we have all the tags in there is the price requried data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11c6722",
   "metadata": {},
   "source": [
    "Now we will extract the text from these tags one by one by looping over these tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ecf2fb82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['56,990',\n",
       " '1,24,000',\n",
       " '1,13,990',\n",
       " '56,990',\n",
       " '84,990',\n",
       " '95,990',\n",
       " '95,890',\n",
       " '96,174',\n",
       " '85,990',\n",
       " '93,990']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in price_tags:\n",
    "    #price=i.text\n",
    "    Price.append(i.text)\n",
    "Price[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec039f7",
   "metadata": {},
   "source": [
    "So,now we have extracted the data requried from the webpage and stored them in the 3 lists mentioned above.Now before creating a dataframe from these lists.Lets first check the length of each of th list. Because if the length of all of the lists are not equal, then a dataframe cannot be formed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "99f34d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 32 30\n"
     ]
    }
   ],
   "source": [
    "print(len(title_names),len(Ratings),len(Price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "df981ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "products=pd.DataFrame({})\n",
    "products['title_names']=title_names[0:10]\n",
    "products['Ratings']=Ratings[0:10]\n",
    "products['Price']=Price[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f3854a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_names</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mi Notebook Horizon Edition 14 Intel Core i7-1...</td>\n",
       "      <td></td>\n",
       "      <td>56,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HP Envy Intel 11th Gen Core i7 Processor 13.3 ...</td>\n",
       "      <td></td>\n",
       "      <td>1,24,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASUS TUF Gaming F17 (2021), 17.3-inch (43.94 c...</td>\n",
       "      <td></td>\n",
       "      <td>1,13,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mi Notebook Horizon Edition 14 Intel Core i7-1...</td>\n",
       "      <td></td>\n",
       "      <td>56,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HP Pavilion (2021) Intel 11th Gen Core i7 14 i...</td>\n",
       "      <td></td>\n",
       "      <td>84,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ASUS TUF Dash F15 (2021), 15.6-inch (39.62 cms...</td>\n",
       "      <td></td>\n",
       "      <td>95,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dell 14 (2021) i7-1195G7 2in1 Touch Screen Lap...</td>\n",
       "      <td></td>\n",
       "      <td>95,890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dell 15 (2021) i7-10870H Gaming Laptop, 16GB D...</td>\n",
       "      <td></td>\n",
       "      <td>96,174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lenovo ThinkBook 15 Intel 11th Gen Core i7 15....</td>\n",
       "      <td></td>\n",
       "      <td>85,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Acer Nitro 5 AN515-57 Gaming Laptop | Intel Co...</td>\n",
       "      <td></td>\n",
       "      <td>93,990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         title_names Ratings     Price\n",
       "0  Mi Notebook Horizon Edition 14 Intel Core i7-1...            56,990\n",
       "1  HP Envy Intel 11th Gen Core i7 Processor 13.3 ...          1,24,000\n",
       "2  ASUS TUF Gaming F17 (2021), 17.3-inch (43.94 c...          1,13,990\n",
       "3  Mi Notebook Horizon Edition 14 Intel Core i7-1...            56,990\n",
       "4  HP Pavilion (2021) Intel 11th Gen Core i7 14 i...            84,990\n",
       "5  ASUS TUF Dash F15 (2021), 15.6-inch (39.62 cms...            95,990\n",
       "6  Dell 14 (2021) i7-1195G7 2in1 Touch Screen Lap...            95,890\n",
       "7  Dell 15 (2021) i7-10870H Gaming Laptop, 16GB D...            96,174\n",
       "8  Lenovo ThinkBook 15 Intel 11th Gen Core i7 15....            85,990\n",
       "9  Acer Nitro 5 AN515-57 Gaming Laptop | Intel Co...            93,990"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f3ae67",
   "metadata": {},
   "source": [
    "Q9: Write a python program to scrape data for first 10 job results for Data Scientist Designation in Noida \n",
    "location. You have to scrape company name, No. of days ago when job was posted, Rating of the company. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "60f1134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4730cf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets now import all the requried librares\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eff64b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets first connect the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\91852\\Dropbox\\My PC (LAPTOP-APF4NTSI)\\Downloads\\chromedriver_win32/chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a4dd42e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "68a49fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.ambitionbox.com/jobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a47c30d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding element for job search bar\n",
    "search_job = driver.find_element_by_xpath('/html/body/div/div/div/div[2]/div[1]/div/div/div/div/span/input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "98f30756",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_job.send_keys(\"Data Scientist\")\n",
    "#search bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e35ae2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn = driver.find_element_by_xpath(\"//span[@class='ctas-btn-medium']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "86b925db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifing the url of the webpage to be scraped\n",
    "url=\"https://www.ambitionbox.com/jobs/search?tag=Data%20Scientist&location=Noida\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "85017717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets open the webpage through our web driver\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d697605",
   "metadata": {},
   "source": [
    "Now we will see in the window opened by webdriver the webpage has opened or not.lets check it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064d907a",
   "metadata": {},
   "source": [
    "So, Now lets first create 10 empty lists.In these lists the data will be stored while scraping.We have created 10 empty lists for 3 features which we have to extract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23006acd",
   "metadata": {},
   "source": [
    "1.company_name 2.job_posted 3.rating_company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f4d72cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_names=[]\n",
    "job_posted=[]\n",
    "ratings_company=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56c4a1e",
   "metadata": {},
   "source": [
    "First,we will extract all the tags where we have the company names.Let me first show you on the webpage in which tags the job titles are put."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f8235772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"f5b9e013716779bd1195d899d8e7d285\", element=\"b1b0f6de-73b1-48a3-9b23-23923c65f3aa\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"f5b9e013716779bd1195d899d8e7d285\", element=\"c829ab25-1058-47ea-a0ca-0c3bca984fb1\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"f5b9e013716779bd1195d899d8e7d285\", element=\"31e4dfa2-52a8-487a-9e0c-696ee16594b7\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"f5b9e013716779bd1195d899d8e7d285\", element=\"a48a507e-b090-475c-b26d-d1ec8fd3c4a1\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"f5b9e013716779bd1195d899d8e7d285\", element=\"5cadb448-0e78-48ad-b9b0-fff24aa4c6c2\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"f5b9e013716779bd1195d899d8e7d285\", element=\"6f223a2e-63c8-419e-81b2-d97405765748\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"f5b9e013716779bd1195d899d8e7d285\", element=\"f54d4163-9f10-4e8f-b416-201667208ff5\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"f5b9e013716779bd1195d899d8e7d285\", element=\"fa3e7ab7-f13c-4bb5-ac20-af49c9a59698\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"f5b9e013716779bd1195d899d8e7d285\", element=\"ac30f1a3-8bf8-4f2b-a142-c89f70ade8fa\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"f5b9e013716779bd1195d899d8e7d285\", element=\"7e78e334-bef3-47a9-b238-9eb4424854b4\")>]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so Lets extract all the tags having the company-names\n",
    "companies_tags=driver.find_elements_by_xpath(\"//a[@class='title noclick']\")\n",
    "companies_tags[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106c11ac",
   "metadata": {},
   "source": [
    "Now we have all the tags in which there are the company names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dfd172",
   "metadata": {},
   "source": [
    "Now we will extract the text from these tags one by one by looping over these tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fc5ceea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data Scientist',\n",
       " 'Urgent Requirement || Data Scientist || Noida',\n",
       " 'Group Lead-Data Scientist',\n",
       " 'Jubilant FoodWorks - Data Scientist - Deep Learning (2-6 yrs)',\n",
       " 'Urgent Vacancy || Data Scientist || Noida',\n",
       " 'Urgent Requirement || Data Scientist || Noida',\n",
       " 'Senior Data Scientist',\n",
       " 'Manager - Data Scientist - Retail/BFSI (8-15 yrs)',\n",
       " 'Data Scientist - Machine Learning (5-14 yrs)',\n",
       " 'Data Scientist - Data Science/Model Development (0-6 yrs)']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now the text of the company names is inside the tags extracted above.\n",
    "# so we will run a loop to iterate over the tags extracted above and extract the tags\n",
    "\n",
    "for i in companies_tags:\n",
    "    companies=i.text\n",
    "    company_names.append(i.text)\n",
    "company_names[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9670a6",
   "metadata": {},
   "source": [
    "Now we will extract all the html tags where we have the job post data. Let me first show you in which tags this data is put on the webpage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "56d1ff94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"f5b9e013716779bd1195d899d8e7d285\", element=\"5ff888a3-37c5-4405-a1c4-944357991d53\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"f5b9e013716779bd1195d899d8e7d285\", element=\"7ecd81f0-5e53-4fdd-8a2b-b9977284bf31\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"f5b9e013716779bd1195d899d8e7d285\", element=\"d41ca21c-1238-440a-a59e-3375cc1a7ea7\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"f5b9e013716779bd1195d899d8e7d285\", element=\"9c47bcb5-5bcf-4aa7-af43-ee5b10987cc9\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"f5b9e013716779bd1195d899d8e7d285\", element=\"5600c5b9-b19e-4cb3-957d-443d20f02167\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"f5b9e013716779bd1195d899d8e7d285\", element=\"37200252-6b0b-4c80-bed4-3157531bd357\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"f5b9e013716779bd1195d899d8e7d285\", element=\"31da8e07-8035-403e-94bb-888523c4b40d\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"f5b9e013716779bd1195d899d8e7d285\", element=\"dbdb6f11-15f6-4d75-bcff-edcca7feb1f9\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"f5b9e013716779bd1195d899d8e7d285\", element=\"61351354-f58e-47e5-87a5-05919bea7595\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"f5b9e013716779bd1195d899d8e7d285\", element=\"d0fe9120-1bb2-43c1-83e0-79b107b1ff84\")>]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so Lets extract all the tags having the job posted data.\n",
    "job_post=driver.find_elements_by_xpath(\"//span[@class='body-small-l']\")\n",
    "job_post[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21642922",
   "metadata": {},
   "source": [
    "Now we have all the tags in there is the job posted data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006c7c07",
   "metadata": {},
   "source": [
    "Now we will extract the text from these tags one by one by looping over these tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aa091671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12d ago',\n",
       " 'via naukri.com',\n",
       " '5d ago',\n",
       " 'via naukri.com',\n",
       " '7d ago',\n",
       " 'via naukri.com',\n",
       " '4d ago',\n",
       " 'via hirist.com',\n",
       " '24d ago',\n",
       " 'via naukri.com']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in job_post:\n",
    "    job=i.text\n",
    "    job_posted.append(i.text)\n",
    "job_posted[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c612e2d0",
   "metadata": {},
   "source": [
    "Now we will extract all the html tags where we have the rating companies data. Let me first show you in which tags this data is put on the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "12c6046b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"f5b9e013716779bd1195d899d8e7d285\", element=\"f95a7339-e705-443e-aa38-21e0e044f1dd\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"f5b9e013716779bd1195d899d8e7d285\", element=\"59ec7c85-7ce5-4b3a-9b50-211457502e2d\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"f5b9e013716779bd1195d899d8e7d285\", element=\"9bad23f8-1506-4203-80be-3ebd43ce2914\")>]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_companies=driver.find_elements_by_xpath(\"//span[@class='body-medium']\")\n",
    "ratings_companies[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9b1223",
   "metadata": {},
   "source": [
    "Now we have all the tags in which there is the data about the rating_company."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef772cb",
   "metadata": {},
   "source": [
    "Now we will extract the text from these tags one by one by looping over these tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6df893e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4.1',\n",
       " '₹ 11L',\n",
       " '₹ 22L',\n",
       " '4.1',\n",
       " '₹ 11L',\n",
       " '₹ 22L',\n",
       " '4.1',\n",
       " '₹ 11L',\n",
       " '₹ 22L',\n",
       " '4.1']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in ratings_companies:\n",
    "    ratings=i.text\n",
    "    ratings_company.append(i.text)\n",
    "ratings_company[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ab3b4e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 20 12\n"
     ]
    }
   ],
   "source": [
    "print(len(company_names),len(job_posted),len(ratings_company))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4089806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['company_names']=company_names[0:10]\n",
    "jobs['job_posted']=job_posted[0:10]\n",
    "jobs['ratings_company']=ratings_company[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "528b0e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_names</th>\n",
       "      <th>job_posted</th>\n",
       "      <th>ratings_company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>12d ago</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Urgent Requirement || Data Scientist || Noida</td>\n",
       "      <td>via naukri.com</td>\n",
       "      <td>₹ 11L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Group Lead-Data Scientist</td>\n",
       "      <td>5d ago</td>\n",
       "      <td>₹ 22L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jubilant FoodWorks - Data Scientist - Deep Lea...</td>\n",
       "      <td>via naukri.com</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Urgent Vacancy || Data Scientist || Noida</td>\n",
       "      <td>7d ago</td>\n",
       "      <td>₹ 11L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Urgent Requirement || Data Scientist || Noida</td>\n",
       "      <td>via naukri.com</td>\n",
       "      <td>₹ 22L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>4d ago</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Manager - Data Scientist - Retail/BFSI (8-15 yrs)</td>\n",
       "      <td>via hirist.com</td>\n",
       "      <td>₹ 11L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist - Machine Learning (5-14 yrs)</td>\n",
       "      <td>24d ago</td>\n",
       "      <td>₹ 22L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist - Data Science/Model Developmen...</td>\n",
       "      <td>via naukri.com</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       company_names      job_posted  \\\n",
       "0                                     Data Scientist         12d ago   \n",
       "1      Urgent Requirement || Data Scientist || Noida  via naukri.com   \n",
       "2                          Group Lead-Data Scientist          5d ago   \n",
       "3  Jubilant FoodWorks - Data Scientist - Deep Lea...  via naukri.com   \n",
       "4          Urgent Vacancy || Data Scientist || Noida          7d ago   \n",
       "5      Urgent Requirement || Data Scientist || Noida  via naukri.com   \n",
       "6                              Senior Data Scientist          4d ago   \n",
       "7  Manager - Data Scientist - Retail/BFSI (8-15 yrs)  via hirist.com   \n",
       "8       Data Scientist - Machine Learning (5-14 yrs)         24d ago   \n",
       "9  Data Scientist - Data Science/Model Developmen...  via naukri.com   \n",
       "\n",
       "  ratings_company  \n",
       "0             4.1  \n",
       "1           ₹ 11L  \n",
       "2           ₹ 22L  \n",
       "3             4.1  \n",
       "4           ₹ 11L  \n",
       "5           ₹ 22L  \n",
       "6             4.1  \n",
       "7           ₹ 11L  \n",
       "8           ₹ 22L  \n",
       "9             4.1  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f498cf9",
   "metadata": {},
   "source": [
    "Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "670ef2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d51060a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets now import all the requried libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10d8bce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets first connect to the web driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\91852\\Dropbox\\My PC (LAPTOP-APF4NTSI)\\Downloads\\chromedriver_win32/chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "373c4fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23276c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b5760c",
   "metadata": {},
   "source": [
    "Now we will see in the window opened by webdriver whether the webpage has opened or not. lets check it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f1a3bd",
   "metadata": {},
   "source": [
    "So,Now lets first create 100 empty lists.In these lists the data will be stored while scraping.We have created 100 empty lists for 3 features which we have to extract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f21902",
   "metadata": {},
   "source": [
    "1.Rating 2.Review_summary 3.Full review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c75b227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating empty list\n",
    "urls=[]\n",
    "short_review=[]\n",
    "complete_review=[]\n",
    "stars=[]\n",
    "time.sleep(2)\n",
    "\n",
    "#scraping 10 pages url\n",
    "url_1 = driver.find_elements_by_xpath(\"//a[@class='ge-49M _2Kfbh8']\")\n",
    "for i in url_1:\n",
    "    urls.append(i.get_attribute('href'))\n",
    "url_2 = driver.find_elements_by_xpath(\"//a[@class='ge-49M']\")\n",
    "for i in url_2:\n",
    "    urls.append(i.get_attribute('href'))\n",
    "time.sleep(4)\n",
    "\n",
    "\n",
    "for i in urls:\n",
    "    driver.get(i)\n",
    "    #for scrapping the number of stars\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='col _2wzgFH K0kLPL']/div[1]/div[1]\"):\n",
    "        stars.append(j.text)\n",
    "    #for scrapping the short review\n",
    "    for k in driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\"):\n",
    "        short_review.append(k.text)\n",
    "    #for scrapping the complete review\n",
    "    for l in driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']/div/div\"):\n",
    "        complete_review.append(l.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae82b694",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining all the lists into a single dataframe\n",
    "df=pd.DataFrame({'Number of Stars': stars,\n",
    "                'Short Review': short_review,\n",
    "               'Full Review': complete_review})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19a9ce84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Stars</th>\n",
       "      <th>Short Review</th>\n",
       "      <th>Full Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>The Best Phone for the Money\\n\\nThe iPhone 11 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Simply awesome</td>\n",
       "      <td>Really satisfied with the Product I received.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Previously I was using one plus 3t it was a gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Best in the market!</td>\n",
       "      <td>Great iPhone very snappy experience as apple k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>5</td>\n",
       "      <td>Super!</td>\n",
       "      <td>This is my first ever iPhone.\\nAnd I truly don...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>5</td>\n",
       "      <td>Just wow!</td>\n",
       "      <td>The ultimate performance\\nCamera is superb\\nTh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>5</td>\n",
       "      <td>Terrific purchase</td>\n",
       "      <td>I use a Note10+ and have been using both iOS a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>5</td>\n",
       "      <td>Awesome</td>\n",
       "      <td>The phone is completely good\\nAs far as camera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>3</td>\n",
       "      <td>Decent product</td>\n",
       "      <td>Everything u ll like it when u use this iPhone...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number of Stars         Short Review  \\\n",
       "0                5            Brilliant   \n",
       "1                5       Simply awesome   \n",
       "2                5     Perfect product!   \n",
       "3                5    Worth every penny   \n",
       "4                5  Best in the market!   \n",
       "..             ...                  ...   \n",
       "85               5               Super!   \n",
       "86               5            Just wow!   \n",
       "87               5    Terrific purchase   \n",
       "88               5              Awesome   \n",
       "89               3       Decent product   \n",
       "\n",
       "                                          Full Review  \n",
       "0   The Best Phone for the Money\\n\\nThe iPhone 11 ...  \n",
       "1   Really satisfied with the Product I received.....  \n",
       "2   Amazing phone with great cameras and better ba...  \n",
       "3   Previously I was using one plus 3t it was a gr...  \n",
       "4   Great iPhone very snappy experience as apple k...  \n",
       "..                                                ...  \n",
       "85  This is my first ever iPhone.\\nAnd I truly don...  \n",
       "86  The ultimate performance\\nCamera is superb\\nTh...  \n",
       "87  I use a Note10+ and have been using both iOS a...  \n",
       "88  The phone is completely good\\nAs far as camera...  \n",
       "89  Everything u ll like it when u use this iPhone...  \n",
       "\n",
       "[90 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7042e38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
